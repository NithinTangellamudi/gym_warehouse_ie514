{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CNN with 25 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.5\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import gym_warehouse\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "env_id = \"warehouse-v0\"\n",
    "\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, info = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return info['image']\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += np.sum(reward)\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "def make_warehouse_cnn(env_id):\n",
    "    env = gym.make(env_id)\n",
    "#     assert 'NoFrameskip' in env.spec.id\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_warehouse_cnn(env_id)\n",
    "state = env.reset()\n",
    "state = np.transpose(state,(2,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('PongNoFrameskip-v0')\n",
    "# from wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
    "# env    = make_atari('PongNoFrameskip-v0')\n",
    "# env    = wrap_deepmind(env)\n",
    "# env    = wrap_pytorch(env)\n",
    "\n",
    "# env.render()\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "out_dir = 'outdir_cnn_1/'\n",
    "\n",
    "\n",
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "gamma      = 0.99\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "#         print(\"STATE: \",state)\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def update_target(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())\n",
    "\n",
    "# estimated_next_q_state_values=[]\n",
    "# estimated_next_q_value=[]\n",
    "\n",
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data.item()\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action\n",
    "\n",
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.savefig(out_dir + 'rewards.png')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.savefig(out_dir + 'losses.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values      = current_model(state)    \n",
    "    next_q_cur_values = current_model(next_state) #next_q_values\n",
    "    next_q_tar_values = target_model(next_state) #next_q_state_values\n",
    "    \n",
    "    q_value       = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value = next_q_tar_values.gather(1, torch.max(next_q_cur_values, 1)[1].unsqueeze(1)).squeeze(1)  \n",
    "    temp = gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    expected_q_value = reward + temp\n",
    "    \n",
    "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean() \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(state)\n",
    "\n",
    "# print(state.shape)\n",
    "\n",
    "# print(np.transpose(state,(2,1,0)).shape)\n",
    "\n",
    "# current_model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "# target_model  = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "\n",
    "current_model = CnnDQN(state.shape, 25)\n",
    "target_model  = CnnDQN(state.shape, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_model = torch.load('outdir/current.model')\n",
    "# current_model.load_state_dict(torch.load('outdir/current.ckpt'))\n",
    "# # target_model  = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "# target_model = torch.load('outdir/target.model')\n",
    "# target_model.load_state_dict(torch.load('outdir/target.ckpt'))\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model = current_model.cuda()\n",
    "    target_model  = target_model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(current_model.parameters(), lr=0.00001)\n",
    "\n",
    "replay_initial = 10000\n",
    "replay_buffer = ReplayBuffer(100000)\n",
    "\n",
    "update_target(current_model, target_model)\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.001\n",
    "epsilon_decay = 1000000\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "num_frames = 1000000\n",
    "batch_size = 32\n",
    "\n",
    "# losses = np.load('outdir/losses.npy')\n",
    "# all_rewards = np.load('outdir/rewards.npy')\n",
    "\n",
    "# losses = losses.tolist()\n",
    "# all_rewards = all_rewards.tolist()\n",
    "losses=[]\n",
    "all_rewards=[]\n",
    "episode_reward = 0\n",
    "\n",
    "# state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame:  10000\n",
      "Episode Reward: 191.99999999997718\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2424880b5f0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mreplay_initial\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mframe_idx\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_td_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-72b3803bdff5>\u001b[0m in \u001b[0;36mcompute_td_loss\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[0mnext_q_tar_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#next_q_state_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mq_value\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "# env.render()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = current_model.act(state, epsilon)\n",
    "    \n",
    "\n",
    "    _, reward, done, info = env.step(action)\n",
    "    next_state = info[\"image\"]\n",
    "    next_state = np.transpose(next_state,(2,1,0))\n",
    "    \n",
    "    \n",
    "#     next_state=next_state.flatten()\n",
    "    \n",
    "#     print(state)\n",
    "\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "#     env.render()\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "\n",
    "    if (frame_idx%100000==0):\n",
    "#         print(\"EPISODE REWARD: \", episode_reward)\n",
    "        # SAVING LOSS AND REWARD\n",
    "        np.save(out_dir + 'losses.npy',np.array(losses))\n",
    "        np.save(out_dir + 'rewards.npy',np.array(all_rewards))\n",
    "\n",
    "#         SAVING CHECKPOINTS\n",
    "        torch.save(current_model.state_dict(),out_dir + 'current.ckpt'.format(frame_idx))\n",
    "        torch.save(target_model.state_dict(),out_dir + 'target.ckpt'.format(frame_idx))\n",
    "\n",
    "#         SAVING MODELS\n",
    "        torch.save(current_model,out_dir + 'current.model')\n",
    "        torch.save(target_model,out_dir + 'target.model')\n",
    "\n",
    "    if frame_idx % 10000==0:\n",
    "        print('Frame: ',frame_idx)\n",
    "        print(\"Episode Reward: {}\".format(episode_reward))\n",
    "#         print(\"ALL REWARDS: {}\".format(sum(all_rewards)))\n",
    "        \n",
    "    if done:\n",
    "        print(\"done\")\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "#         print(\"Episode Reward When Done: {}\".format(episode_reward))\n",
    "        episode_reward = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if (len(replay_buffer) > replay_initial) and (frame_idx%4==0):\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if frame_idx % 100000 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)\n",
    "\n",
    "    if frame_idx % 100000 == 0:\n",
    "        update_target(current_model, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
